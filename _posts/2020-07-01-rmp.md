---
title: "What Makes A Good Professor?"
date: 2020-07-01
tags: [python, web scraping, natural language processing, machine learning]
header:
  image: "/images/rmp_images/ucd_library.jpg"
excerpt: "Natural Language Processing, Web Scraping, Machine Learning"
mathjax: "true"
---
## Introduction

When I was in college, it was crucial that I had a good professor to succeed in the class. Unlike many of my friends who could easily self-learn the class, I was the kid who was at every office hours asking questions from start to finish. As a result, it was drastically important that I could have one-on-one time with someone who could explain the material to my liking.

One tool that I used throughout my college career was the review site, *RateMyProfessors*. Here, I could go through hundreds of reviews to find the best professors for a class in a given quarter. Personally, RMP did a pretty good job at giving an overall rundown of what to expect from a professor. Going into a class where I knew the professor was highly rated, I rarely was let down. In this project, I wanted to validate my personal experience with the actual data.

## Goals

I wanted to answer a couple of different questions with this analysis:

* **How were students talking about my old professors?**
* **What do students look for in a good professor?**
* **Which schools in the UC system get more positive reviews?**

Apart from this exploratory analysis, I also hoped to **develop a model that can classify whether a review will be good or bad**.

## Tools (Python)

* **Data Collection:** *BeautifulSoup*
* **Natural Language Processing:** *CountVectorizer, re, TextBlob*
* **Data Manipulation/Pre-processing:** *Pandas, NumPy*
* **Data Visualization:** *Matplotlib, Seaborn, WordCloud*
* **Machine Learning:** *Sklearn*

## Results

***How were students talking about my old professors?***

After compiling the reviews for some of my old professors, I found the words/short phrases (tokens) that were commonly found in their reviews, as shown in the WordClouds below! The left side shows the most frequent tokens for one of my best professors at UC Davis, who has an overall quality of 4.5 (out of 5). The flip side shows a professor whose class I performed poorly in. From personal experience, I think these were fairly accurate when describing these professors and their classes. It was interesting to see the shift in tone between a seemingly "good" professor and "bad" professor.

<img src="{{ site.url }}{{ site.baseurl }}/images/rmp_images/wordcloud_copy.png" alt="wordcloud">

***What do students look for in a good professor?***

When I finished the previous section, I wanted to dive deeper into *what makes a professor "good"* at my school. The first task was to define "good."

<img src="{{ site.url }}{{ site.baseurl }}/images/rmp_images/quality_dist_copy.png" alt="dist">

After determining the **distribution of overall quality was approximately normal**, I classified **all professors who had an overall quality that was one standard deviation more than the mean (3.75) to be "good"** The "cutoff" to be a good professor was an overall quality of **4.45**.

| Tag       | Percentage of Good Professors With Tag|
| ------------- |:-------------:|
| Respected| 80% |
| Amazing lectures| 70% |   
| Inspirational | 50% |
| Hilarious | 40% |
| Caring | 40% |

The first detail we examined was the **frequently used tags** used for these good professors. Interestingly, 4 of the 5 most frequently seen tags were **not related to actual teaching style**. Surprisingly, **the majority of frequently seen tags were related to personality!**

To further investigate this result, I again looked at the text of the review and found the most frequent tokens found with the good professors. Based on token frequency, it appears that **students also value teaching style**, with words like '*clear*' and '*helpful*' being prevalent. We can also see that **students may actually be valuing quality over easiness**. There's a good mix of words like "*easy*" and "*hard"*, "*helpful*" and "*difficult*", and words that indicate that you still have to do work to succeed like "*study*", "*homework*", etc..

<img src="{{ site.url }}{{ site.baseurl }}/images/rmp_images/good_prof_copy.png" alt="wordcloud">

***Do some schools in the UC system get better reviews than others?***

In this part, I decided to take a deeper look into the text of the reviews. Within each text, I examined its **polarity and subjectivity** as defined by [`TextBlob`](https://textblob.readthedocs.io/en/dev/). Specifically looking across two departments (Statistics and Economics) at three different UC's (UC Davis, UC Irvine, UC Berkeley), it appears that each school follows a similar pattern in distribution.

<img src="{{ site.url }}{{ site.baseurl }}/images/rmp_images/schools_copy.png" alt="dist">

On average, all of the schools had **slightly positive toned reviews**, but followed a normal distribution. Similarly, each of the schools had **highly subjective ratings**, which makes sense for a student's personal review. From this, we can see that **there was no one school in the UC system that got significantly "better" reviews than the other.**

***Classification***

In the final part of the project, I wanted to wrap things up by creating a model that could classify whether a review was going to be good (quality > 2.5) or bad based on its contents.

| Model      | 10-fold CV Accuracy|
| ------------- |:-------------:|
| Logistic Regression| 84.10% |
| Gradient Boosting Classifier| 83.88% |   
| Gaussian Naive Bayes| 82.40% |
| K-Nearest Neighbors | 77.89% |
| Linear Support Vector Classifier | 77.01% |
| Decision Tree Classifier | 76.00% |
| Stochastic Gradient Descent | 75.64 % |

It looks like the Logistic Regression model was able to classify a review with about 84% accuracy. That's pretty good!

You can find the Jupyter Notebook to my work [here!](https://nbviewer.jupyter.org/github/tylerchang23/ratemyprof/blob/master/RMP_FINAL.ipynb)
